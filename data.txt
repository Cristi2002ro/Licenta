import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow import keras
from tensorflow.python.framework import ops

# Încarcă dataset-ul
def load_dataset():
    # Încarcă fișierul CSV ce conține întrebările și răspunsurile
    df = pd.read_csv("codebase_qa_dataset.csv")
    
    # Verifică și curăță datele
    print("Verificare NaN în dataset:")
    print(df.isnull().sum())  # Vezi dacă sunt valori NaN
    
    # Elimină rândurile cu NaN
    df = df.dropna()
    
    # Asigură-te că datele sunt de tip string
    df['question'] = df['question'].astype(str)
    df['answer'] = df['answer'].astype(str)

    return df

# Preprocesare text
def preprocess_text(df):
    tokenizer = tf.keras.preprocessing.text.Tokenizer()  # Folosim doar pentru tokenizare
    tokenizer.fit_on_texts(df['question'])
    
    # Tokenizează întrebările și răspunsurile
    X = tokenizer.texts_to_sequences(df['question'])
    y = tokenizer.texts_to_sequences(df['answer'])
    
    # Adaugă padding pentru a face secvențele de lungime uniformă
    X = tf.keras.preprocessing.sequence.pad_sequences(X, padding='post')
    y = tf.keras.preprocessing.sequence.pad_sequences(y, padding='post')

    # Convertim răspunsurile în format categoric (pentru softmax)
    y = tf.keras.utils.to_categorical(y, num_classes=len(tokenizer.word_index) + 1)
    
    return X, y, tokenizer

# Definirea modelului fără Keras
def build_model(input_length, output_length, vocab_size):
    model = tf.Graph()
    
    with model.as_default():
        # Definirea unui loc pentru input
        inputs = tf.placeholder(tf.float32, [None, input_length], name="inputs")
        labels = tf.placeholder(tf.float32, [None, output_length], name="labels")
        
        # Definirea unui loc pentru embedding
        with tf.variable_scope("embedding"):
            embedding_matrix = tf.Variable(tf.random_uniform([vocab_size, 100], -1.0, 1.0), name="embedding_matrix")
            embedded_input = tf.nn.embedding_lookup(embedding_matrix, inputs)
        
        # LSTM Layer
        with tf.variable_scope("lstm"):
            lstm_cell = tf.nn.rnn_cell.LSTMCell(100)
            outputs, _ = tf.nn.dynamic_rnn(lstm_cell, embedded_input, dtype=tf.float32)
            lstm_output = outputs[:, -1, :]
        
        # Fully Connected Layer
        with tf.variable_scope("dense"):
            dense_layer = tf.layers.dense(lstm_output, 100, activation=tf.nn.relu)
        
        # Output layer with softmax
        with tf.variable_scope("output"):
            output_layer = tf.layers.dense(dense_layer, output_length, activation=tf.nn.softmax)
        
        # Cost function and optimizer
        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output_layer, labels=labels))
        optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)
        
    return model, inputs, labels, output_layer, cost, optimizer

# Antrenează modelul
def train_model():
    # 1. Încarcă dataset-ul
    df = load_dataset()
    
    # 2. Preprocesează textul
    X, y, tokenizer = preprocess_text(df)

    # 3. Împărțirea datelor în seturi de antrenament și test
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # 4. Învăță vocabularul din date
    vocab_size = len(tokenizer.word_index) + 1  # +1 pentru tokenul '0' (pad)
    
    # 5. Ajustează dimensiunea de ieșire pentru răspunsuri
    output_length = y.shape[1]  # Lungimea secvenței de răspunsuri (pentru softmax)
    
    # 6. Creează modelul
    model, inputs, labels, output_layer, cost, optimizer = build_model(X_train.shape[1], output_length, vocab_size)
    
    # 7. Antrenează modelul
    with tf.Session(graph=model) as session:
        session.run(tf.global_variables_initializer())
        
        for epoch in range(10):  # Pentru fiecare epocă
            _, training_cost = session.run([optimizer, cost], feed_dict={inputs: X_train, labels: y_train})
            print(f"Epoch {epoch+1}, Training cost: {training_cost}")
        
        # 8. Salvează modelul (Pentru acest exemplu, nu folosim model.save, salvăm doar variabilele)
        saver = tf.train.Saver()
        saver.save(session, "qa_model.ckpt")
        print("Modelul a fost salvat ca 'qa_model.ckpt'.")

if __name__ == "__main__":
    train_model()
